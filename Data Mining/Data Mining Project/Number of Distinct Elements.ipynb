{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flajolet-Martin algorithm\n",
    "\n",
    "We will implement the Flajolet Martin algorithm for counting the number of distinct elements in a stream or a very large data. Refer to the <a href=\"https://drive.google.com/file/d/18uY18kqGPMcW67bAjNiltTk6sSr6Wh2B/view?usp=sharing\">slides</a> for the details. \n",
    "\n",
    "As in the case of bloom filter, we will use the MurmurHash 3 as our hash function, with different seeds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import mmh3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "The algorithm uses several hash functions, each of which sends every element to a bitarray of size $nBits$. The hash functions are divided into $nGroup$ groups, each group having $nHash$ hash functions. \n",
    "\n",
    "Each hash function keeps track of the maximum tail length $R$ seen so far. The final estimate is obtained by combining the $2^R$ values from the different hash functions. \n",
    "\n",
    "<b>Note:</b> The MMDS book suggests taking average of each group and median of the averages. However, in my experiments (and in Wikipedia too), I found the other way works better: median of the groups and average of the medians. In this implementation, the two approaches are implemented as estimate1 and estimate2 respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlajoletMartinEstimator(object):\n",
    "    \n",
    "    def __init__(self, nBits, nHash, nGroups): \n",
    "        self.nBits = nBits\n",
    "        self.nHash = nHash\n",
    "        self.nGroups = nGroups\n",
    "        self.maxTailLength = np.zeros((self.nHash, self.nGroups))\n",
    "        #print(self.maxTailLength)\n",
    "        \n",
    "    \n",
    "    def tail_length(self, num_str):\n",
    "        return len(num_str)-len(num_str.rstrip('0')) \n",
    "    \n",
    "    \n",
    "    def fmHash(self, item,i):\n",
    "        return bin(mmh3.hash(item,i) % 2**self.nBits)\n",
    "    \n",
    "    \n",
    "    def estimate1(self):\n",
    "        # Average of 2^R within each group \n",
    "        # and median of the estimates from the groups\n",
    "        averages = np.mean(2**self.maxTailLength, axis=0)\n",
    "        # print('averages:', averages)\n",
    "        return np.median(averages)\n",
    "    \n",
    "    def estimate2(self):\n",
    "        # Median of 2^R within each group \n",
    "        # and average of the estimates from the groups\n",
    "        medians = np.median(2**self.maxTailLength, axis=0)\n",
    "        #print('medians:', medians)\n",
    "        return np.mean(medians)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def add(self, item):\n",
    "        for i in range(self.nHash):\n",
    "            for j in range(self.nGroups):    \n",
    "                hash_seq = i * self.nHash + j\n",
    "                # print('Hash number: ', hash_seq)\n",
    "                bitstring = self.fmHash(item, hash_seq)\n",
    "                tailLength = self.tail_length(bitstring)\n",
    "                if tailLength > self.maxTailLength[i,j]:\n",
    "                    self.maxTailLength[i,j] = tailLength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the algorithm\n",
    "\n",
    "We will test our algorithm against actual number of distinct elements using a dictionary (which will be slow if the number of distinct elements is very large). This is surely not the ideal way to use this algorithm (when exact counting is possible). Ideally, one should use it for estimating the number of distinct elements in gigabytes of data (you may want to try that with a huge text corpus). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary for counting actual number of distinct elements\n",
    "distinctElements = {}\n",
    "\n",
    "# Our FM-estimator\n",
    "fm = FlajoletMartinEstimator(64,5,6)\n",
    "\n",
    "for i in range(10000):\n",
    "    a = str(np.random.randint(100000))\n",
    "    fm.add(a)\n",
    "    distinctElements[a] = 1\n",
    "# print(fm.maxTailLength)\n",
    "print(\"Estimate 1: \", fm.estimate1())\n",
    "print(\"Estimate 2: \", fm.estimate2())\n",
    "print(\"Actual: \", len(distinctElements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
